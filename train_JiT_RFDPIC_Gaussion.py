# train_lightning.py (ä¿®æ”¹ç‰ˆ)

# ... (æ‰€æœ‰ import ä¿æŒä¸å˜, åŒ…æ‹¬ RFDPIC çš„) ...
import os
import argparse
import yaml
import torch
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from transformers import get_linear_schedule_with_warmup
from collections import OrderedDict
import torchmetrics
import torch.nn as nn
from einops import rearrange
import numpy as np
import json
from evaluation.fvd.torchmetrics_wrap import FrechetVideoDistance
from evaluation.psd.psd_metric import PSDAverageMetric
from evaluation.psd.psd_tools import plot_sample_psd, calculate_psd_error_metrics

from models.MotionPredictor.RFDPIC_Dual_Rotation_dyn import RFDPIC_Dual_Rotation_Dyn
from utils.transform import data_transform, inverse_data_transform
from dataset_btchw import Himawari8LightningDataModule
from models.ViT import JiT
from videoMeanflow_RFDPIC_JiT_Gaussion import MeanFlow 
from visualize import vis_himawari8_seq_btchw, plot_metrics_curve


class VideoLightningModule(pl.LightningModule):
    # ... (load_frozen_rfdpic å’Œ configure_optimizers ä¿æŒä¸å˜) ...

    def __init__(self, model_config, data_config, meanflow_config, optimizer_config, scheduler_config, training_config, logging_config, eval_config,
                 rfdpic_config_path: str,
                 rfdpic_ckpt_path: str,
                 sample_steps: int):
        
        super().__init__()
        self.save_hyperparameters()
        
        # 1. åŠ è½½ RFDPIC (ä¸å˜)
        print(f"Loading RFDPIC config from: {rfdpic_config_path}")
        with open(rfdpic_config_path, 'r') as f:
            rfdpic_main_cfg = yaml.safe_load(f) 
            self.rfdpic_rescaled = rfdpic_main_cfg.get('dataset', {}).get('rescaled', True)
            rfdpic_base_dir = os.path.dirname(rfdpic_config_path)
            model_cfg_path = rfdpic_main_cfg.get('model', {}).get('rf_dp_config_file')
            self.rfdpic_model_cfg_path = os.path.join(rfdpic_base_dir, model_cfg_path)
            if not os.path.exists(self.rfdpic_model_cfg_path):
                self.rfdpic_model_cfg_path = model_cfg_path
            print(f"Using RFDPIC model config: {self.rfdpic_model_cfg_path}")
        self.rfdpic_model = self.load_frozen_rfdpic(
            rfdpic_config_path=rfdpic_config_path,
            rfdpic_ckpt_path=rfdpic_ckpt_path
        )

        # 2. å®ä¾‹åŒ– MeanFlow (ä½¿ç”¨æ–°ä¿®æ”¹çš„ videoMeanflow.py)
        #    __init__ æœ¬èº«ä¸éœ€è¦æ”¹åŠ¨
        self.meanflow = MeanFlow(
            channels=model_config['out_channels_c'],
            time_dim=model_config['input_size'][0],
            height_dim=model_config['input_size'][1],
            width_dim=model_config['input_size'][2],
            normalizer=['minmax', None, None],
            flow_ratio=meanflow_config['flow_ratio'],
            time_dist=meanflow_config['time_dist'],
            cfg_ratio=meanflow_config['cfg_ratio'],
            cfg_scale=meanflow_config['cfg_scale'],
            cfg_uncond=meanflow_config['cfg_uncond']
        )
        
        self.sample_steps = sample_steps
        print(f"--- Using {self.sample_steps} sampling steps for prediction ---")

        # 3. å®ä¾‹åŒ– U-Net
        # ğŸ”´ è¿™é‡Œçš„ model_config['in_channels_c'] å¿…é¡»æ˜¯ 6 (ç”± main() ä¼ å…¥)
        print(f"Initializing UNet with condition input_t (in_channels_c): {model_config['in_channels_c']}")
        self.model = JiT(
            input_size=tuple(model_config['input_size']),
            in_channels_c=model_config['in_channels_c'],  # æ¡ä»¶è¾“å…¥é€šé“æ•° (6)
            out_channels_c=model_config['out_channels_c'], # é¢„æµ‹è¾“å‡ºé€šé“æ•° (8)
            time_emb_dim=model_config['time_emb_dim'],
            patch_size=model_config['patch_size'],
            hidden_size=model_config['hidden_size'],
            depth=model_config['depth'],
            num_heads=model_config['num_heads'],
            mlp_ratio=model_config['mlp_ratio'],
            bottleneck_dim=model_config['bottleneck_dim'],
        )
        
        self.val_loader_iter = None

        # ğŸ”´ 4. æ–°å¢ï¼šåˆå§‹åŒ–æ‰€æœ‰æµ‹è¯•æŒ‡æ ‡
        # ä»é…ç½®ä¸­è·å–é€šé“æ•°å’Œæ—¶é—´æ­¥æ•°
        self.num_channels = self.hparams.model_config['out_channels_c']
        # (B, 6, C, H, W) -> 6 æ­¥
        self.num_timesteps = 6 
        
        # æŒ‡æ ‡ (åå½’ä¸€åŒ–)
        self.test_d_mse = torchmetrics.MeanSquaredError()
        self.test_d_mae = torchmetrics.MeanAbsoluteError()

        # æŒ‡æ ‡ (å½’ä¸€åŒ–, data_range=1.0)
        self.test_ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=1.0)
        self.test_psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=1.0)
        
        # é€é€šé“ã€é€æ—¶é—´æ­¥æŒ‡æ ‡ (åå½’ä¸€åŒ–)
        # ä½¿ç”¨ ModuleList åŒ…è£…ï¼Œä½¿å…¶èƒ½è¢« .to(device) è‡ªåŠ¨ç§»åŠ¨
        self.test_d_mse_metric = nn.ModuleList([
            nn.ModuleList([torchmetrics.MeanSquaredError() for _ in range(self.num_timesteps)]) 
            for _ in range(self.num_channels)
        ])
        self.test_d_mae_metric = nn.ModuleList([
            nn.ModuleList([torchmetrics.MeanAbsoluteError() for _ in range(self.num_timesteps)]) 
            for _ in range(self.num_channels)
        ])
        self.test_d_rmse_metric = nn.ModuleList([
            nn.ModuleList([torchmetrics.MeanSquaredError(squared=False) for _ in range(self.num_timesteps)]) 
            for _ in range(self.num_channels)
        ])
        
        # FVD æŒ‡æ ‡
        self.test_fvd_list = nn.ModuleList([
            FrechetVideoDistance(feature=400, normalize=False) 
            for _ in range(self.num_channels)
        ])

        # ğŸ”´ æ–°å¢ï¼š(å›ç­”æ‚¨çš„æŒ‡æ ‡è¦æ±‚ 1)
        # åƒ FVD ä¸€æ ·ï¼Œæˆ‘ä»¬å®šä¹‰ä¸‰ä¸ªæµå¼æŒ‡æ ‡ï¼Œå®ƒä»¬ä¸ä¼šçˆ†å†…å­˜
        H_W = self.hparams.model_config['input_size'][1] # åº”è¯¥æ˜¯ 256
        self.test_psd_gt = PSDAverageMetric(H=H_W, W=H_W)
        self.test_psd_pred = PSDAverageMetric(H=H_W, W=H_W)
        self.test_psd_rfdpic = PSDAverageMetric(H=H_W, W=H_W)
        
        # å¯è§†åŒ–index
        self.train_example_data_idx_list = eval_config['train_example_data_idx_list']
        self.val_example_data_idx_list = eval_config['val_example_data_idx_list']
        self.test_example_data_idx_list = eval_config['test_example_data_idx_list']

        # ğŸ”´ 4. æ–°å¢ï¼šä» train_UNet_RFDPIC.py ç…§æ¬ç»Ÿè®¡æ•°æ®åŠ è½½é€»è¾‘
        with open(data_config['train_json_path'], 'r') as f:
            train_global_stats = json.load(f)
        train_global_max_values = torch.tensor(train_global_stats['Global Max'], 
                                                   dtype=torch.float32).reshape(1, 1, 8, 1, 1)
        train_global_min_values = torch.tensor(train_global_stats['Global Min'], 
                                                   dtype=torch.float32).reshape(1, 1, 8, 1, 1)
        train_global_range = train_global_max_values - train_global_min_values
        self.register_buffer('train_global_min_values', train_global_max_values)
        self.register_buffer('train_global_min_values', train_global_min_values)
        self.register_buffer('train_global_range', train_global_range)
        
        with open(data_config['test_json_path'], 'r') as f:
            test_global_stats = json.load(f)
        test_global_max_values = torch.tensor(test_global_stats['Global Max'], 
                                                   dtype=torch.float32).reshape(1, 1, 8, 1, 1)
        test_global_min_values = torch.tensor(test_global_stats['Global Min'], 
                                                   dtype=torch.float32).reshape(1, 1, 8, 1, 1)
        test_global_range = test_global_max_values - test_global_min_values
        self.register_buffer('test_global_min_values', test_global_max_values)
        self.register_buffer('test_global_min_values', test_global_min_values)
        self.register_buffer('test_global_range', test_global_range)
        
        self.plot_dict = {
            'train': {'max': train_global_stats['Global Max'], 'min': train_global_stats['Global Min']},
            'val': {'max': train_global_stats['Global Max'], 'min': train_global_stats['Global Min']},
            'test': {'max': test_global_stats['Global Max'], 'min': test_global_stats['Global Min']}
        }

    # ğŸ”´ æ–°å¢ï¼šè¦†ç›–æ­¤æ–¹æ³•ä»¥å…è®¸ä»æ—§ checkpoint æ¢å¤
    def load_state_dict(self, state_dict, strict: bool = True):
        """
        è¦†ç›– Pytorch Lightning çš„é»˜è®¤è¡Œä¸ºã€‚
        æˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨ strict=Falseï¼Œè¿™æ ·åœ¨åŠ è½½ä¸€ä¸ªæ—§çš„ã€
        æ²¡æœ‰ FVD æˆ– global_stats ç¼“å†²åŒºçš„ checkpoint æ—¶ï¼Œ
        å®ƒä¸ä¼šå› ä¸º "Missing key(s)" é”™è¯¯è€Œå´©æºƒã€‚
        
        è¿™ä½¿å¾— `trainer.fit(ckpt_path=...)` èƒ½å¤ŸæˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼Œ
        åŒæ—¶ä¹Ÿèƒ½æ­£ç¡®æ¢å¤ optimizerã€scheduler å’Œ global_stepã€‚
        """
        # å¼ºåˆ¶ä½¿ç”¨ strict=False æ¥å¿½ç•¥ç¼ºå¤±çš„é”® (FVD, global_stats ç­‰)
        super().load_state_dict(state_dict, strict=False)
    
    def load_frozen_rfdpic(self, rfdpic_config_path, rfdpic_ckpt_path):
        # ... (æ­¤è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜) ...
        print(f"Loading pretrained RFDPIC model from {rfdpic_ckpt_path}")
        with open(rfdpic_config_path, 'r') as f:
            rfdpic_cfg = yaml.safe_load(f)['model']
        model = RFDPIC_Dual_Rotation_Dyn(
            dp_name=rfdpic_cfg['dp_name'],
            rf_name=rfdpic_cfg['rf_name'],
            rf_dp_config_file=self.rfdpic_model_cfg_path,
            dp_mode=rfdpic_cfg['dp_mode'],
            rf_mode=rfdpic_cfg['rf_mode'],
            alpha=rfdpic_cfg['alpha'],
            interpolation_mode=rfdpic_cfg.get('interpolation_mode', "bilinear"),
            padding_mode=rfdpic_cfg.get('padding_mode', "border"),
            use_res_scale=rfdpic_cfg.get('use_res_scale', False),
            res_scale_init=rfdpic_cfg.get('res_scale_init', 0.05),
            advect_mode=rfdpic_cfg.get('advect_mode', 'semi-lagrangian'),
            ode_method=rfdpic_cfg.get('ode_method', 'rk4'),
            ode_substeps=rfdpic_cfg.get('ode_substeps', 1),
            use_splat=rfdpic_cfg.get('use_splat', False),
            splat_type=rfdpic_cfg.get('splat_type', 'softmax'),
            splat_alpha=rfdpic_cfg.get('splat_alpha', 20.0),
            splat_blend=rfdpic_cfg.get('splat_blend', 0.5),
            use_gate=rfdpic_cfg.get('use_gate', False),
            use_implicit_diffusion=rfdpic_cfg.get('use_implicit_diffusion', False),
            jacobi_iters=rfdpic_cfg.get('jacobi_iters', 2),
            D_max=rfdpic_cfg.get('D_max', 0.2),
            k_edge=rfdpic_cfg.get('k_edge', 0.1),
            diff_dt=rfdpic_cfg.get('diff_dt', 1.0),
            diff_dx=rfdpic_cfg.get('diff_dx', 1.0),
            D_const=rfdpic_cfg.get('D_const', None),
        )
        state_dict = torch.load(rfdpic_ckpt_path, map_location="cpu", weights_only=False)
        if "state_dict" in state_dict:
            state_dict = state_dict["state_dict"]
        model_state_dict = OrderedDict()
        for key, val in state_dict.items():
            if key.startswith("model."):
                model_state_dict[key.replace("model.", "", 1)] = val
            else:
                model_state_dict[key] = val
        model.load_state_dict(model_state_dict)
        model.eval()
        for param in model.parameters():
            param.requires_grad = False
        print(f"{'='*10} RFDPIC model loaded and frozen. {'='*10}")
        return model

    # ğŸ”´ æ–°å¢ï¼šåå½’ä¸€åŒ–è¾…åŠ©å‡½æ•°
    def denormalize(self, data: torch.Tensor, mode: str=''):
        if mode == 'train' or mode == 'val':
            # (æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¿é—®åœ¨ __init__ ä¸­æ³¨å†Œçš„ 'train' buffer)
            return data * self.train_global_range + self.train_global_min_values
        elif mode == 'test':
            # (æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¿é—®åœ¨ __init__ ä¸­æ³¨å†Œçš„ 'test' buffer)
            return data * self.test_global_range + self.test_global_min_values
        else:
            raise NotImplementedError

    def configure_optimizers(self):
        # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.optimizer_config['lr'],
            weight_decay=self.hparams.optimizer_config['weight_decay']
        )
        scheduler = get_linear_schedule_with_warmup(
            optimizer=optimizer,
            num_warmup_steps=self.hparams.scheduler_config['num_warmup_steps'],
            num_training_steps=self.hparams.training_config['n_steps']
        )
        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "step", "frequency": 1}}

    def training_step(self, batch, batch_idx):
        c_past, x_future = batch # (B, 6, C, H, W), (B, 6, C, H, W)
        
        with torch.no_grad():
            c_past_norm = data_transform(c_past, rescaled=self.rfdpic_rescaled)
            c_rfdpic_norm, _, _, _, _ = self.rfdpic_model(c_past_norm)
            c_rfdpic = inverse_data_transform(c_rfdpic_norm, rescaled=self.rfdpic_rescaled)
            c_rfdpic = c_rfdpic.detach() # (B, 6, C, H, W)

        # --- ğŸ”´ 1. ä¿®æ”¹ï¼šå°† c ä½œä¸ºå…ƒç»„ (c_start, c_cond) ä¼ é€’ ---
        c_cond_cat = torch.cat([c_past, c_rfdpic], dim=2)
        loss, mse_val = self.meanflow.loss(self.model, x_future, c=(c_rfdpic, c_cond_cat))
        
        self.log('train/loss', loss, on_step=True, on_epoch=False, prog_bar=True)
        self.log('train/mse_loss', mse_val, on_step=True, on_epoch=False)
        self.log('learning_rate', self.trainer.optimizers[0].param_groups[0]['lr'], on_step=True, on_epoch=False)
        
        return loss

    def on_train_batch_end(self, outputs, batch, batch_idx):
        save_freq = self.hparams.logging_config['save_step_frequency']
        
        if self.trainer.is_global_zero and self.global_step > 0 and self.global_step % save_freq == 0:
            self.model.eval()
            
            if self.val_loader_iter is None:
                self.val_loader_iter = iter(self.trainer.datamodule.val_dataloader())
            try:
                val_batch = next(self.val_loader_iter)
            except StopIteration:
                self.val_loader_iter = iter(self.trainer.datamodule.val_dataloader())
                val_batch = next(self.val_loader_iter)

            c_past_val, x_future_val = val_batch
            c_past_val = c_past_val.to(self.device)
            x_future_val = x_future_val.to(self.device)

            with torch.no_grad():
                # --- ğŸ”´ 2. ä¿®æ”¹ï¼šä¸ºé‡‡æ ·å‡†å¤‡ (c_start, c_cond) ---
                c_past_norm_val = data_transform(c_past_val, rescaled=self.rfdpic_rescaled)
                c_rfdpic_norm_val, _, _, _, _ = self.rfdpic_model(c_past_norm_val)
                c_rfdpic_val = inverse_data_transform(c_rfdpic_norm_val, rescaled=self.rfdpic_rescaled)
                
                c_cond_cat_val = torch.cat([c_past_val, c_rfdpic_val], dim=2) 
                c_tuple_val = (c_rfdpic_val, c_cond_cat_val) # <-- ä¼ å…¥ (c_start, c_cond_cat)

                z = self.meanflow.sample_prediction(
                    self.model, 
                    c_tuple_val, # <-- ä¼ å…¥å…ƒç»„
                    sample_steps=self.sample_steps,
                    device=self.device
                )

            # ... (å¯è§†åŒ–å’Œæ¸…ç†ä¸å˜) ...
            c_past_sample = c_past_val[0].cpu()
            z_sample = z[0].cpu()
            x_future_sample = x_future_val[0].cpu()
            c_rfdpic_sample = c_rfdpic_val[0].cpu()
            del c_past_val, x_future_val, z, val_batch, c_rfdpic_val, c_tuple_val
            torch.cuda.empty_cache()
            context_list = [c_past_sample[t] for t in range(c_past_sample.shape[0])]
            pred_list = [z_sample[t] for t in range(z_sample.shape[0])]
            target_list = [x_future_sample[t] for t in range(x_future_sample.shape[0])]
            pred_rfdpic_list = [c_rfdpic_sample[t] for t in range(c_rfdpic_sample.shape[0])]
            save_dir = os.path.join(self.trainer.logger.save_dir, "images", f"step_{self.global_step}")
            vis_himawari8_seq_btchw(save_dir=save_dir, context_seq=context_list, pred_seq=pred_list, target_seq=target_list)
            vis_himawari8_seq_btchw(save_dir=os.path.join(self.trainer.logger.save_dir, "images", f"step_{self.global_step}_rfdpic_cond"), context_seq=context_list, pred_seq=pred_rfdpic_list, target_seq=target_list)
            self.model.train()

    # ğŸ”´ æ–°å¢ï¼šåœ¨æµ‹è¯•å¼€å§‹æ—¶åˆ›å»ºç›®å½•
    def on_test_epoch_start(self):
        if self.trainer.is_global_zero:
            self.metrics_save_dir = os.path.join(self.trainer.logger.save_dir, "metrics")
            os.makedirs(self.metrics_save_dir, exist_ok=True)
            self.example_save_dir = os.path.join(self.trainer.logger.save_dir, "examples_test")
            os.makedirs(self.example_save_dir, exist_ok=True)

    # ğŸ”´ ä¿®æ”¹ï¼šå®Œæ•´çš„ test_step
    @torch.no_grad()
    def test_step(self, batch, batch_idx):
        c_past, x_future = batch
        
        # --- 1. è·å– RFDPIC é¢„æµ‹ (c_start) ---
        c_past_norm = data_transform(c_past, rescaled=self.rfdpic_rescaled)
        c_rfdpic_norm, _, _, _, _ = self.rfdpic_model(c_past_norm)
        c_rfdpic = inverse_data_transform(c_rfdpic_norm, rescaled=self.rfdpic_rescaled)
        c_rfdpic = c_rfdpic.detach()

        # --- 2. è·å– MeanFlow é¢„æµ‹ (å½’ä¸€åŒ–) ---
        c_cond_cat = torch.cat([c_past, c_rfdpic], dim=2) 
        c_cond_cat = c_cond_cat.detach()
        c_tuple = (c_rfdpic, c_cond_cat) # <-- ä¼ å…¥ (c_start, c_cond_cat)
        
        # preds_norm æ˜¯ (B, 6, C, H, W) å¹¶ä¸”æ˜¯å½’ä¸€åŒ–çš„ (0-1 èŒƒå›´)
        preds_norm = self.meanflow.sample_prediction(
            self.model, 
            c_tuple,
            sample_steps=self.sample_steps, # æ³¨æ„ï¼šé‡‡æ ·æ­¥æ•°åº”ä¸éªŒè¯æ—¶ä¸€è‡´
            device=self.device
        )
        targets_norm = x_future
        
        # --- 3. åå½’ä¸€åŒ–ç”¨äºè®¡ç®— MSE/MAE/RMSE ---
        preds_denorm = self.denormalize(preds_norm, mode='test')
        targets_denorm = self.denormalize(targets_norm, mode='test')
        
        # --- 4. è®¡ç®—æŒ‡æ ‡ ---
        # æ€»ä½“æŒ‡æ ‡ (åœ¨åå½’ä¸€åŒ–æ•°æ®ä¸Š)
        self.test_d_mse(preds_denorm, targets_denorm)
        self.test_d_mae(preds_denorm, targets_denorm)
        
        # SSIM/PSNR (åœ¨å½’ä¸€åŒ–æ•°æ®ä¸Š)
        # (B, T, C, H, W) -> (B*T, C, H, W)
        preds_bchw = rearrange(preds_norm, 'b t c h w -> (b t) c h w')
        targets_bchw = rearrange(targets_norm, 'b t c h w -> (b t) c h w')
        self.test_ssim(preds_bchw, targets_bchw)
        self.test_psnr(preds_bchw, targets_bchw)
        
        # ğŸŸ¢ æŒ‰ç…§ä½ çš„æè®®ï¼šæ‹¼æ¥ (Cat) è¾“å…¥å’Œè¾“å‡ºæ¥åˆ›å»º T=12 çš„è§†é¢‘
        # c_past, targets_norm, å’Œ preds_norm æ­¤æ—¶éƒ½åœ¨ GPU ä¸Š
        # (B, 6, C, H, W) + (B, 6, C, H, W) -> (B, 12, C, H, W)
        real_video_T12 = torch.cat([c_past, targets_norm], dim=1)
        fake_video_T12 = torch.cat([c_past, preds_norm], dim=1)
        
        # ğŸŸ¢ ä¿®æ”¹ï¼šå¾ªç¯éå† 8 ä¸ªé€šé“ï¼Œåˆ†åˆ«æ›´æ–° FVD æŒ‡æ ‡
        for c in range(self.num_channels):
            # real_video_T12 æ˜¯ (B, 12, 8, H, W)
            # æˆ‘ä»¬æå–ç¬¬ c ä¸ªé€šé“ï¼Œå¾—åˆ° (B, 12, 1, H, W)
            real_ch_video = real_video_T12[:, :, c:c+1, :, :]
            fake_ch_video = fake_video_T12[:, :, c:c+1, :, :]
            
            # è¿™é‡Œçš„ FVD æŒ‡æ ‡ä¼šè‡ªåŠ¨å¤„ç† C=1 -> C=3
            self.test_fvd_list[c].update(real_ch_video, real=True)
            self.test_fvd_list[c].update(fake_ch_video, real=False)

        # ğŸ”´ æ›´æ–° PSD æŒ‡æ ‡ (å…¨å±€)
        # è¿™å°±åƒ FVD.update()ï¼Œå®ƒåªç´¯åŠ æ€»å’Œï¼Œä¸ä¿å­˜æ•°æ®ï¼Œä¸ä¼šçˆ†å†…å­˜
        self.test_psd_gt.update(targets_norm)
        self.test_psd_pred.update(preds_norm)
        self.test_psd_rfdpic.update(c_rfdpic) # (B, 6, C, H, W)

        # é€é€šé“/é€æ—¶é—´æ­¥æŒ‡æ ‡ (åœ¨åå½’ä¸€åŒ–æ•°æ®ä¸Š)
        for c in range(self.num_channels):
            for t in range(self.num_timesteps):
                self.test_d_mae_metric[c][t](preds_denorm[:,t,c].contiguous(), targets_denorm[:,t,c].contiguous())
                self.test_d_mse_metric[c][t](preds_denorm[:,t,c].contiguous(), targets_denorm[:,t,c].contiguous())
                self.test_d_rmse_metric[c][t](preds_denorm[:,t,c].contiguous(), targets_denorm[:,t,c].contiguous())

        # --- 5. å¯è§†åŒ– (ä¾‹å¦‚ï¼šå‰ 4 ä¸ª batch) ---
        # Visualiozation
        micro_batch_size = c_past.shape[0]
        data_idx = int(batch_idx * micro_batch_size)
        if data_idx in self.test_example_data_idx_list:
            c_past_sample = c_past[0].cpu()
            preds_sample = preds_norm[0].cpu() # å¯è§†åŒ–å½’ä¸€åŒ–æ•°æ®
            targets_sample = targets_norm[0].cpu() 
            
            context_list = [c_past_sample[t] for t in range(c_past_sample.shape[0])]
            pred_list = [preds_sample[t] for t in range(preds_sample.shape[0])]
            target_list = [targets_sample[t] for t in range(targets_sample.shape[0])]
            
            save_dir = os.path.join(self.example_save_dir, f"test_sample_{data_idx}")
            vis_himawari8_seq_btchw(
                save_dir=save_dir, 
                context_seq=context_list, 
                pred_seq=pred_list, 
                target_seq=target_list
            )
            
            # ğŸ”´ æ–°å¢ï¼š(å›ç­”æ‚¨çš„å¯è§†åŒ–è¦æ±‚ 2)
            # è°ƒç”¨æˆ‘ä»¬ç®€å•çš„â€œä¸€ä½“åŒ–â€ç»˜å›¾å‡½æ•°
            try:
                rfdpic_sample_np = c_rfdpic[0].cpu().numpy() # (T, C, H, W)
                preds_sample_np = preds_sample.numpy()
                targets_sample_np = targets_sample.numpy()
                
                # è°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Œå®Œæˆæ‰€æœ‰è®¡ç®—å’Œç»˜å›¾
                plot_sample_psd(
                    pred_4d=preds_sample_np,
                    rfdpic_4d=rfdpic_sample_np,
                    gt_4d=targets_sample_np,
                    save_dir=save_dir
                )

            except Exception as e:
                print(f"Failed to generate sample PSD plot for sample {data_idx}: {e}")
        

    def on_test_epoch_end(self):
        # èšåˆæ‰€æœ‰ GPU ä¸Šçš„æŒ‡æ ‡
        test_d_mse = self.test_d_mse.compute()
        test_d_mae = self.test_d_mae.compute()
        test_ssim = self.test_ssim.compute()
        test_psnr = self.test_psnr.compute()
        
        
        # è®°å½•æ—¥å¿—
        self.log('test/d_mse_epoch', test_d_mse, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        self.log('test/d_mae_epoch', test_d_mae, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        self.log('test/ssim_epoch', test_ssim, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        self.log('test/psnr_epoch', test_psnr, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        

        # é‡ç½®æŒ‡æ ‡
        self.test_d_mse.reset()
        self.test_d_mae.reset()
        self.test_ssim.reset()
        self.test_psnr.reset()
        
        # ğŸ”´ æ–°å¢ï¼šå¾ªç¯è®¡ç®—ã€è®°å½•å’Œé‡ç½® 8 ä¸ªé€šé“çš„ FVD
        all_fvd_values = [] 
        for c in range(self.num_channels):
            try:
                test_fvd_c = self.test_fvd_list[c].compute()
                # æ—¥å¿—åç§°ç±»ä¼¼: test/fvd_epoch_ch0, test/fvd_epoch_ch1, ...
                # self.log(f'test/fvd_epoch_ch{c}', test_fvd_c, prog_bar=False, on_step=False, on_epoch=True, sync_dist=True)
                # è®°å½•æˆåŠŸè®¡ç®—çš„FVDå€¼
                all_fvd_values.append(test_fvd_c)
            except Exception as e:
                print(f"Could not compute FVD for channel {c}: {e}")
            
            self.test_fvd_list[c].reset()
        
        if all_fvd_values: # ç¡®ä¿åˆ—è¡¨ä¸­æœ‰å€¼ï¼Œé˜²æ­¢é™¤ä»¥é›¶
            # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡
            all_fvd_tensor = torch.stack(all_fvd_values) 
            # è®¡ç®—å¹³å‡å€¼
            mean_fvd = all_fvd_tensor.mean() 
            # è®°å½•æ•´ä½“çš„å¹³å‡FVD
            self.log('test/fvd_epoch_mean', mean_fvd, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        else:
            print("\n--- Warning: No FVD values were successfully computed for averaging. ---\n")

        # ğŸ”´ æ–°å¢ï¼šè®¡ç®—ã€è®°å½•å’Œé‡ç½® PSD æŒ‡æ ‡
        # .compute() ä¼šè‡ªåŠ¨ DDP åŒæ­¥ï¼Œè¿”å›å…¨å±€å¹³å‡æ›²çº¿
        k_axis_gt, psd_gt = self.test_psd_gt.compute()
        _, psd_pred = self.test_psd_pred.compute()
        _, psd_rfdpic = self.test_psd_rfdpic.compute()
        
        # è½¬æ¢ä¸º NumPy ç”¨äºæŒ‡æ ‡è®¡ç®—
        k_axis_np = k_axis_gt.cpu().numpy()
        psd_gt_np = psd_gt.cpu().numpy()
        psd_pred_np = psd_pred.cpu().numpy()
        psd_rfdpic_np = psd_rfdpic.cpu().numpy()

        # é‡ç½® (åƒ FVD.reset())
        self.test_psd_gt.reset()
        self.test_psd_pred.reset()
        self.test_psd_rfdpic.reset()

        # (ä»…åœ¨ rank 0 ä¸Šä¿å­˜å’Œç»˜å›¾)
        if self.trainer.is_global_zero:
            
            # ğŸ”´ æ–°å¢ï¼šè°ƒç”¨ PSD æŒ‡æ ‡è®¡ç®— (Part 2)
            try:
                # æŒ‰ç…§æ‚¨çš„è¦æ±‚è®¾ç½®ç™¾åˆ†æ¯”
                splits_list = [0.50, 0.75, 0.90, 0.95] # å¯¹åº” top 50%, 25%, 10%, 5%
                
                # è®¡ç®— MeanFlow vs GT
                psd_metrics_pred = calculate_psd_error_metrics(
                    psd_gt_np, psd_pred_np, k_axis_np, splits_list
                )
                print("\n--- PSD Metrics (MeanFlow Pred vs. GT) ---")
                for key, value in psd_metrics_pred.items():
                    self.log(f'test/psd_pred_{key}', value, prog_bar=False, on_step=False, on_epoch=True, sync_dist=False)
                    print(f"  {key}: {value:.4f}")
                
                # è®¡ç®— RFDPIC vs GT (ä½œä¸ºå¯¹æ¯”)
                psd_metrics_rfdpic = calculate_psd_error_metrics(
                    psd_gt_np, psd_rfdpic_np, k_axis_np, splits_list
                )
                print("\n--- PSD Metrics (RFDPIC vs. GT) ---")
                for key, value in psd_metrics_rfdpic.items():
                    self.log(f'test/psd_rfdpic_{key}', value, prog_bar=False, on_step=False, on_epoch=True, sync_dist=False)
                    print(f"  {key}: {value:.4f}")

            except Exception as e:
                print(f"Failed to compute PSD metrics: {e}")

        # å¤„ç†é€é€šé“æŒ‡æ ‡ (ä»…åœ¨ rank 0 ä¸Šæ‰§è¡Œ)
        if self.trainer.is_global_zero:
            d_mse_metric = np.zeros((self.num_channels, self.num_timesteps))
            d_mae_metric = np.zeros((self.num_channels, self.num_timesteps))
            d_rmse_metric = np.zeros((self.num_channels, self.num_timesteps))
            
            # éå†æ‰€æœ‰é€šé“å’Œæ—¶é—´æ­¥
            for c in range(self.num_channels):
                for t in range(self.num_timesteps):
                    # è®¡ç®— MSE
                    mse = self.test_d_mse_metric[c][t].compute()
                    d_mse_metric[c][t] = mse.cpu().item()
                    self.test_d_mse_metric[c][t].reset()  # é‡ç½®
                    # è®¡ç®— MAE
                    mae = self.test_d_mae_metric[c][t].compute()
                    d_mae_metric[c][t] = mae.cpu().item()
                    self.test_d_mae_metric[c][t].reset()  # é‡ç½®
                    # è®¡ç®— RMSE
                    rmse = self.test_d_rmse_metric[c][t].compute()
                    d_rmse_metric[c][t] = rmse.cpu().item()
                    self.test_d_rmse_metric[c][t].reset()  # é‡ç½®
            
            # ä¿å­˜åˆ° CSV
            np.savetxt(f"{self.metrics_save_dir}/mse.csv", d_mse_metric, delimiter=",")
            np.savetxt(f"{self.metrics_save_dir}/mae.csv", d_mae_metric, delimiter=",")
            np.savetxt(f"{self.metrics_save_dir}/rmse.csv", d_rmse_metric, delimiter=",")
            
            # ç»˜åˆ¶æ›²çº¿å›¾
            plot_metrics_curve(self.metrics_save_dir, "mse", d_mse_metric)
            plot_metrics_curve(self.metrics_save_dir, "mae", d_mae_metric)
            plot_metrics_curve(self.metrics_save_dir, "rmse", d_rmse_metric)
            print(f"Test metrics saved and plotted in {self.metrics_save_dir}")


def main():
    # ... (parser å’Œ args ä¸å˜, ä»ç„¶éœ€è¦ --rfdpic_config å’Œ --rfdpic_ckpt) ...
    parser = argparse.ArgumentParser(description="PyTorch Lightning Video Prediction Training")
    parser.add_argument('--config', type=str, default="config.yaml", help="Path to the MeanFlow config.yaml file")
    parser.add_argument('--log_dir', type=str, default="./logs", help="Directory to save logs and checkpoints")
    parser.add_argument('--batch_size', type=int, default=None, help="Batch size (overrides config if set)")
    parser.add_argument('--mode', type=str, default="train", choices=["train", "test"], help="Training or testing mode")
    parser.add_argument('--ckpt_path', type=str, default=None, help="Path to checkpoint for resuming MeanFlow training or testing")
    parser.add_argument('--gpus', type=int, default=1, help="Number of GPUs to use")
    parser.add_argument('--rfdpic_config', type=str, required=True, help="Path to the RFDPIC main config.yaml")
    parser.add_argument('--rfdpic_ckpt', type=str, required=True, help="Path to the RFDPIC model checkpoint (.pt or .ckpt)")
    parser.add_argument('--sample_steps', type=int, default=10, help="Number of sampling steps for MeanFlow (overrides config if set)")
    parser.add_argument('--use_wandb', action='store_true', help="Use WandbLogger (default: False)")
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    pl.seed_everything(42, workers=True)

    # 4. åˆå§‹åŒ– DataModule (ä¸å˜)
    dataset_cfg = config['data']
    datamodule = Himawari8LightningDataModule(
        dataset_name=dataset_cfg["dataset_name"],
        train_data_path=dataset_cfg["train_data_path"],
        train_json_path=dataset_cfg["train_json_path"],
        train_dataset_prefix=dataset_cfg["train_dataset_prefix"],
        train_ratio=dataset_cfg["train_ratio"],
        train_random_flip=dataset_cfg["train_random_flip"],
        test_data_path=dataset_cfg["test_data_path"],
        test_json_path=dataset_cfg["test_json_path"],
        test_dataset_prefix=dataset_cfg["test_dataset_prefix"],
        batch_size=args.batch_size,
        num_workers=dataset_cfg["num_workers"],
        pin_memory=dataset_cfg["pin_memory"],
    )

    # 5. åˆå§‹åŒ– LightningModule (ä¸å˜)
    model = VideoLightningModule(
        model_config=config['model'],
        data_config=config['data'],
        meanflow_config=config['meanflow'],
        optimizer_config=config['optimizer'],
        scheduler_config=config['scheduler'],
        training_config=config['training'],
        logging_config=config['logging'],
        eval_config=config['eval'],
        rfdpic_config_path=args.rfdpic_config,
        rfdpic_ckpt_path=args.rfdpic_ckpt,
        sample_steps=args.sample_steps,
    )

    # ... (Logger, Checkpoint, Trainer, trainer.fit/test å‡ä¿æŒä¸å˜) ...
    # ğŸ”´ ä¿®æ”¹ Logger åˆå§‹åŒ–é€»è¾‘
    if args.use_wandb:
        print("--- Using Weights & Biases Logger ---")
        logger_instance = WandbLogger(
            project=config['logging']['project_name'], 
            save_dir=args.log_dir, 
            name=os.path.basename(args.log_dir)
        )
        logger_instance.watch(model, log="all", log_freq=500)
    else:
        print(f"--- WandbLogger is disabled, using TensorBoardLogger at {args.log_dir} ---")
        # ğŸ”´ æ˜ç¡®åˆ›å»º TensorBoardLogger å¹¶æŒ‡å®š save_dir
        # æˆ‘ä»¬è®¾ç½® name="" æ¥é¿å…å¤šä½™çš„ "default" å­ç›®å½•
        # æ—¥å¿—å°†ä¿å­˜åœ¨: args.log_dir/version_X
        logger_instance = TensorBoardLogger(
            save_dir=args.log_dir,
            name="" 
        )

    checkpoint_callback = ModelCheckpoint(
        dirpath=os.path.join(args.log_dir, "checkpoints"),
        filename="step_{step:06d}-loss_{train/loss:.4f}",
        every_n_train_steps=config['logging']['save_step_frequency'],
        save_top_k=-1,
        auto_insert_metric_name=False
    )
    
    trainer = pl.Trainer(
        logger=logger_instance, # <-- ç°åœ¨ logger_instance å§‹ç»ˆæ˜¯ä¸€ä¸ªé…ç½®å¥½çš„ logger
        callbacks=[checkpoint_callback],
        max_steps=config['training']['n_steps'],
        devices=args.gpus,
        accelerator="gpu",
        precision=32,
        log_every_n_steps=config['logging']['log_every_n_steps'],
        gradient_clip_val=config['training']['gradient_clip_val']
    )
    # if args.mode == 'train':
    #     print(f"--- Starting Training ---")
    #     print(f"Log dir: {args.log_dir}")
    #     ckpt_to_resume = None
    #     if args.ckpt_path:
    #         print(f"--- æ­£åœ¨ä» MeanFlow checkpoint å»¶ç»­è®­ç»ƒ: {args.ckpt_path} ---")
    #         ckpt_to_resume = args.ckpt_path
    #     else:
    #         print(f"--- ä»å¤´å¼€å§‹è®­ç»ƒ MeanFlow ---")
    #     trainer.fit(model, datamodule, ckpt_path=ckpt_to_resume)
    if args.mode == 'train':
        print(f"--- Starting Training ---")
        print(f"Log dir: {args.log_dir}")
        ckpt_to_resume = None
        if args.ckpt_path:
            # ğŸŸ¢ ç°åœ¨è¿™ä¸ªå¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼
            print(f"--- æ­£åœ¨ä» MeanFlow checkpoint å»¶ç»­è®­ç»ƒ: {args.ckpt_path} ---")
            ckpt_to_resume = args.ckpt_path
        else:
            print(f"--- ä»å¤´å¼€å§‹è®­ç»ƒ MeanFlow ---")
        
        # ğŸŸ¢ Lightning ä¼šè‡ªåŠ¨åŠ è½½ ckpt_to_resume
        # å®ƒä¼šè°ƒç”¨æˆ‘ä»¬è¦†ç›–çš„ load_state_dict(..., strict=False)
        # å¹¶ä¸”å®ƒè¿˜ä¼šæˆåŠŸåŠ è½½ optimizer, scheduler, å’Œ global_step
        trainer.fit(model, datamodule, ckpt_path=ckpt_to_resume)
    elif args.mode == 'test':
        if args.ckpt_path is None:
            raise ValueError("Must provide --ckpt_path for testing.")
            
        print(f"--- Starting Testing ---")
        print(f"--- Manually loading checkpoint with strict=False: {args.ckpt_path} ---")
        # 1. æ‰‹åŠ¨åŠ è½½ checkpoint
        # æ­¤æ—¶ model (VideoLightningModule) å·²ç»è¢«åˆå§‹åŒ–äº†,
        # å®ƒä» JSON ä¸­è¯»å–çš„ buffer (train_global_min_values ç­‰) å·²ç»å­˜åœ¨ã€‚
        checkpoint = torch.load(args.ckpt_path, map_location="cpu")

        # 2. ä½¿ç”¨ strict=False åŠ è½½ state_dict
        # è¿™å°†åŠ è½½æ‰€æœ‰åŒ¹é…çš„ key (ä¾‹å¦‚ U-Net çš„æƒé‡)
        # å¹¶è‡ªåŠ¨å¿½ç•¥ checkpoint ä¸­ç¼ºå¤±çš„ buffer keys (train_global_min_values ç­‰)
        # model ä¸­å·²ç»ä» JSON åŠ è½½çš„ buffer å€¼å°†è¢«ä¿ç•™ï¼Œä¸ä¼šè¢«è¦†ç›–ã€‚
        model.load_state_dict(checkpoint["state_dict"], strict=False)
        
        # 3. è°ƒç”¨ trainer.testï¼Œä½†ä¸ä¼ å…¥ ckpt_path
        # å› ä¸ºæ¨¡å‹çŠ¶æ€å·²ç»æ‰‹åŠ¨åŠ è½½å®Œæ¯•
        trainer.test(model, datamodule)

if __name__ == '__main__':
    main()